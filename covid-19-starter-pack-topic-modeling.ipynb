{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Simple Starter Pack Code and an Introduction to Topic Modeling with LDA\n\n![](https://2.bp.blogspot.com/-UO8E6wws1Go/XGWgbLTPJnI/AAAAAAAABoQ/tGuBrjfJZ1UGmUQ112ZCv3gAu3Tg0O1FACLcBGAs/s1600/image001-)","metadata":{}},{"cell_type":"markdown","source":"The following code contains a basic overview of how perform topic modeling with the CORD-19-research-challenge data. It contains a JSON file reader object, language detection, text processing, and Latent Dirichlet Allocation (LDA) Topic Modeling, with a visualization of the different sets of topics at the end of the notebook.","metadata":{}},{"cell_type":"code","source":"#importing necessary libraries \n\nimport json\nimport glob\nimport re\nimport string\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom random import sample\n\n\n# adding a label to each column that specifies the nature of the data in that column.\ncol_list = ['cord_uid', 'sha', 'source_x', 'title', 'doi', 'pmcid', 'pubmed_id', 'license', 'publish_time',\n            'authors', 'journal', 'url']\nmetadata = pd.read_csv('../input/CORD-19-research-challenge/metadata.csv')\n\n#creating another df with a more output-friendly view for exploration\nmetadata_view = pd.read_csv('../input/CORD-19-research-challenge/metadata.csv', usecols = col_list)\n","metadata":{"_uuid":"228582f9-bec0-4496-9515-5c730dc2a92b","_cell_guid":"0f915b9b-b31f-46ca-8c4b-93cd4e54b8ec","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# High Level View of Data","metadata":{}},{"cell_type":"code","source":"# gives few rows of dataframe\nmetadata_view.head()","metadata":{"_uuid":"427f17d8-3bba-4e21-a175-b7073add31e7","_cell_guid":"06a934f9-d559-4722-8496-9e2e9ac88d5d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gives column count and other info of original metadata df\nmetadata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I noticed quite a bit of null values as I went through the csv. Let's check how many null abstracts there are. Let's check all the columns.","metadata":{}},{"cell_type":"code","source":"print(f\"{metadata['abstract'].isnull().sum()} papers with no abstracts provided\")\nprint(metadata.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that 13043 records in metadata have a null value for 'sha', which actually contains the paper_id we need to match to...\n\n186 records without a title, 3370 with no doi, 8277 with no abstract, etc.\none way to deal with this issue is to match on doi, which only contains 3370 nulls (less than 13043 null 'sha') The only issue is that 'doi' is found in random areas of each JSON file.","metadata":{}},{"cell_type":"code","source":"# here we'll load the JSON files by gathering the path to all JSON files\n# glob module allows for reading of all JSON files in the All JSON folder\n\nall_json = glob.glob('../input/CORD-19-research-challenge/document_parses/**/*.json',\n                     recursive=True)\n\n# creating a reader class to read the files\nclass JSONFileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            # paper_id\n            self.paper_id = content['paper_id']\n            # title and initializing abstract and body_text attributes\n            self.title = content['metadata'].get(\"title\")\n            self.abstract = []\n            self.body_text = []\n            # abstract\n            for word in content['abstract']:\n                self.abstract.append(word.get(\"text\"))\n            # body_text\n            for word in content['body_text']:\n                self.body_text.append(word.get(\"text\"))\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n            \n        # repr function gives a printable version of a given object\n        def __repr__(self):\n            return f'{self.paper_id}: {self.abstract[:100]}... {self.body_text[:100]}...'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining a data dictionary before reading through the files will help us input each attribute in each column as we iterate through the list of files\ndatadict = {'paper_id': [], 'title': [], 'doi': [], 'journal': [], 'authors': [], 'abstract': [], 'body_text': []}\n\n# iterating and reading JSON files, selecting a sample for runtime purposes may be helpful\nfor file in sample(all_json,20000):\n    try:\n        reader = JSONFileReader(file)\n        # you may see here that I am comparing the 'sha' column to the JSON file's paper_id using 'contains'. This is because some of the 'sha' column entries\n        # include multiple paper_id entries\n        sub = metadata[metadata['sha'].str.contains(reader.paper_id, na = False, flags = re.IGNORECASE, regex = False)]\n        datadict['paper_id'].append(reader.paper_id)\n        datadict['title'].append(reader.title)\n        doilist = list(sub['doi'])\n        # checking if the list is null, and setting it to N/A rather than the nan type, which is always annoying to deal with (python being a bad friend).\n        if not doilist:\n            doilist.append('N/A')\n        journallist = list(sub['journal'])\n        if not journallist:\n            journallist.append('N/A')\n        datadict['doi'].append(doilist[0])\n        datadict['journal'].append(journallist[0])\n        authorlist = list(sub['authors'])\n        if not authorlist:\n            authorlist.append('N/A')\n        datadict['authors'].append(authorlist[0])\n        if not list(reader.abstract):\n            reader.abstract = 'N/A'\n        datadict['abstract'].append(reader.abstract)\n        if not list(reader.body_text):\n            reader.body_text = 'N/A'\n        datadict['body_text'].append(reader.body_text)\n    # always remember to set an exception clause\n    except Exception as e:\n        continue\n\n# create dataframe\ndata = pd.DataFrame(datadict, columns = ['paper_id', 'title','doi','journal','authors','abstract','body_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checked for duplicates. We can safely assume there are little to no duplicates in the JSON list and it is clean\nprint(f\" {len(data)} before\")\ndata.drop_duplicates()\nprint(f\" {len(data)} after\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LANGUAGE DETECTION\n!pip install langdetect","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langdetect import detect\nfrom langdetect import DetectorFactory\n\n# set seed\nDetectorFactory.seed = 0\n\nlanglist = []\n\nfor i in range(0, len(data)):\n    # split by space into list, take the first x index, separate by space\n    text = data.iloc[i]['body_text'].split(\" \")\n\n    lang = \"en\"\n    all_words = set(text)\n    try:\n        lang = detect(\" \".join(all_words))\n    except Exception as e:\n        try:\n            lang = detect(data.iloc[i]['abstract_summary'])\n        except Exception as e:\n            lang = \"unknown\"\n            pass\n    # get the language\n    langlist.append(lang)\n\nlanguage_dict = {}\nfor x in langlist:\n    language_dict[x] = langlist.count(x)\n\nprint(set(langlist))\n\n# not all sources are in english, for our purposes here let's limit use to only english sources\n# create and assign a column of languages, limit use to only the english papers\n\ndata['languages'] = langlist\ndata = data[data['languages']=='en']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TEXT PROCESSING\n\nfrom nltk.tokenize import word_tokenize\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n# we need to remove punctuation and convert all words to their lowercase. \n# In addition, I added custom stopwords for words that appear consistently but do not provide value to the analysis. For example, 'PMC' refers to the PubMed Central archive, which does not provide value to topic modeling.\n\n# remove punctuation\npunctuations = string.punctuation\npunc = list(punctuations)\n\ncustom_stop = ['``', ':', '/', 'N/A', 'etc.', 'it', 'The', 'For', ';', 'his', 'her', 'you', 'an', 'at', 'be', 'they', 'or', 'on', 'them', 'these', 'into', 'from', 'while', 'this', 'also', 'was', 'with', 'not', 'to', 'in', 'their', \"''\", 'are', 'by', 'per', 'as', 'is', 'that', 'the', 'and', 'of', 'a', 'for', 'doi', 'preprint', 'copyright', 'org', 'https', 'et', 'al', 'author', 'figure', 'table', 'http',\n    'rights', 'I', 'biorxiv', 'medrxiv', '19', 'used', 'using', 'license', 'fig', 'fig.', 'al.', 'infl', 'uenza', 'Elsevier', 'PMC', 'CZI', '-PRON-', 'usually', '10']\n\n# appending custom stopwords and punctuation\ncustom_stop.append(STOP_WORDS)\n\ntemp = []\n\nfor x in range(0, len(data)):\n    text = data.iloc[x]['body_text']\n    text_tokenized = word_tokenize(text)\n    for i in range(0, len(text_tokenized)):\n        text_tokenized[i] = text_tokenized[i].lower()\n    text_tokenized = [word for word in text_tokenized if word not in custom_stop and word not in punc]\n    temp.append(\" \".join(text_tokenized))\n\ntoken_final = pd.DataFrame(temp, columns=['body_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Topic Modeling\n\nThe first part of this sections contains a function that looks into the top 10 words","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\nimport pyLDAvis\nfrom pyLDAvis import sklearn as sklearn_lda\n\ncount_vectorizer = CountVectorizer(stop_words='english')\n\ndef plot_10_most_common_words(count_data, count_vectorizer):\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts += t.toarray()[0]\n\n    count_dict = (zip(words, total_counts))\n    count_dict = sorted(count_dict, key=lambda x: x[1], reverse=True)[0:10]\n    words = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    x_pos = np.arange(len(words))\n\n    plt.figure(2, figsize=(15, 15 / 1.6180))\n    plt.subplot(title='10 most common words')\n    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n    sns.barplot(x_pos, counts, palette='husl')\n    plt.xticks(x_pos, words)\n    plt.xlabel('words')\n    plt.ylabel('counts')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualising the 10 most common words\n\ncount_data = count_vectorizer.fit_transform(token_final['body_text'].values)\nplot_10_most_common_words(count_data, count_vectorizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LDA\n\nLDA is a form of topic modeling, which is an unsupervised classification of documents. Topic Modeling allows us to discover structure, classify sdocumntents according to this structure, and perform an action based on that classification. LDA, or latent dirichlet allocation,works by assuming that the way a document was generated was by picking a set of topics and then for each topic was used to pick a set of words.\n\nPyLDAvis is a handy library for visualizing an LDA model. The only paramter that needs selection is the number of overall topics, which I have set to be around 5.","metadata":{}},{"cell_type":"code","source":"# counting the frequencies of each word\ncountvector = CountVectorizer(strip_accents='unicode', stop_words = 'english', lowercase=True,\n                                  token_pattern=r'\\b[a-zA-Z]{3,}\\b')\ndtm_tf = countvector.fit_transform(token_final['body_text'].values)\nprint(dtm_tf.shape)\n\n# associated articles with 5 unique topics but fuzzy associations (multiple words may appear in multiple topic spaces)\nn_topics = 5\nlda_tf = LDA(n_components=n_topics, max_iter=10, learning_method='online',\n             verbose=False, random_state=42)\nlda_tf.fit(dtm_tf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LDAvis = sklearn_lda.prepare(lda_tf, dtm_tf, countvector)\npyLDAvis.display(LDAvis)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}